##############################################################################
# Copyright (c) Members of the EGEE Collaboration. 2004. 
# See http://www.eu-egee.org/partners/ for details on the copyright 
# holders.  
#
# Licensed under the Apache License, Version 2.0 (the "License"); 
# you may not use this file except in compliance with the License. 
# You may obtain a copy of the License at 
#
#    http://www.apache.org/licenses/LICENSE-2.0 
#
# Unless required by applicable law or agreed to in writing, software 
# distributed under the License is distributed on an "AS IS" BASIS, 
# WITHOUT WARRANTIES OR CONDITIONS 
# OF ANY KIND, either express or implied. 
# See the License for the specific language governing permissions and 
# limitations under the License.
##############################################################################
#
# NAME :        site-info.def
#
# DESCRIPTION : This is the main configuration file needed to execute the
#               yaim command. It contains a list of the variables needed to
#               configure a service.
#
# AUTHORS :     yaim-contact@cern.ch
#
# NOTES :       - site-info.def will contain the list of variables common to 
#                 multiple node types. Node type specific variables are 
#                 distributed by its corresponding module although a unique 
#                 site-info.def can still be used at configuration time. 
#               
#               - Service specific variables are be distributed under 
#                    /opt/glite/yaim/examples/siteinfo/services/<node_type_name>
#                 Copy this file under you siteinfo/services directory or also copy the variables
#                 manually in site-info.def. 
#                 DPM and LFC variables are not yet distributed in their
#                 corresponding YAIM modules. Find the list of relevant variables in: 
#                    - https://twiki.cern.ch/twiki/bin/view/LCG/Site-info_configuration_variables#DPM  
#                    - https://twiki.cern.ch/twiki/bin/view/LCG/Site-info_configuration_variables#LFC
#
#               - site-info.pre and site-info.post contain default variables. When sys admins
#                 want to set their own values, they can just define the variable in site-info.def
#                 and that will overwrite the value in site-info.pre/post.
#
#               - VO variables for LCG VOs are currently distributed with example values. 
#                 For up to date information of any VO please check the CIC portal VO ID Card information:
#                 http://cic.in2p3.fr/ 
#
#               - For more information on YAIM, please check:
#                 https://twiki.cern.ch/twiki/bin/view/EGEE/YAIM
#
#               - For a detailed description of site-info.def variables, please check:
#                 https://twiki.cern.ch/twiki/bin/view/LCG/Site-info_configuration_variables#site_info_def
#   
# YAIM MODULE:  glite-yaim-core
#                 
##############################################################################

###################################
# General configuration variables #
###################################
YAIM_LOGGING_LEVEL=DEBUG

# This is the dir where the config files for gLite are located
GLITE_DIR=/root/conf/glite
MY_DOMAIN=portable-grid.de

# List of the batch nodes hostnames and optionally the subcluster ID the
# WN belongs to. An example file is available in 
# ${INSTALL_ROOT}/glite/yaim/examples/wn-list.conf
# Change the path according to your site settings.
WN_LIST=${GLITE_DIR}/wn-list.conf

# List of unix users to be created in the service nodes.
# The format is as follows:
# UID:LOGIN:GID1,GID2,...:GROUP1,GROUP2,...:VO:FLAG:
# An example file is available in ${INSTALL_ROOT}/glite/yaim/examples/users.conf
# Change the path according to your site settings.
# For more information please check ${INSTALL_ROOT}/glite/yaim/examples/users.conf.README 
USERS_CONF=${GLITE_DIR}/users.conf

# List of the local accounts which a user should be mapped to.
# The format is as follows:
# "VOMS_FQAN":GROUP:GID:FLAG:[VO]
# An example file is available in ${INSTALL_ROOT}/glite/yaim/examples/groups.conf
# Change the path according to your site settings.
# For more information please check ${INSTALL_ROOT}/glite/yaim/examples/groups.conf.README
# NOTE: comment out this variable if you want to specify a groups.conf per VO
# under the group.d/ directory.
GROUPS_CONF=${GLITE_DIR}/groups.conf

# Uncomment this variable if you want to specify a local groups.conf 
# It is similar to GROUPS_CONF but used to specify a separate file
# where local accounts specific to the site are defined.
# LOCAL_GROUPS_CONF=my_local_groups.conf

# Uncomment this variable if you are installing a mysql server
# It is the MySQL admin password. 
MYSQL_PASSWORD=portablegrid

# Uncomment this variable if you want to explicitely use pool
# accounts for special users when generating the grid-mapfile.
# If not defined, YAIM will decide whether to use special 
# pool accounts or not automatically
# SPECIAL_POOL_ACCOUNTS=yes or no


################################
# Site configuration variables #
################################

# Human-readable name of your site
SITE_NAME=PowerGrid
SITE_LOC="Goettingen, Germany"

# The contact e-mail of your site.
# A coma separated list of email addresses. 
SITE_EMAIL="Gen.Kawamura@cern.ch"

# It is the position of your site north or south of the equator 
# measured from -90. to 90. with positive values going north and 
# negative values going south. 
SITE_LAT=50.0 

# It is the position of the site east or west of Greenwich, England 
# measured from -180. to 180. with positive values going east and 
# negative values going west.  
SITE_LONG=50.0

# Uncomment this variable if your site has an http proxy
# in order to reduce the load on the CA host
# SITE_HTTP_PROXY="http-proxy.my.domain"

################################
# User configuration variables #
################################

# Uncomment the following variables if you want to create system user
# accounts under a HOME directory different from /home.
# Note: It is recommendable to use /var/lib/user_name as HOME directory for
# system users.
# EDG_HOME_DIR=/var/lib/edguser
# EDGINFO_HOME_DIR=/var/lib/edginfo
# BDII_HOME_DIR=/var/lib/edguser

##############################
# CE configuration variables #
##############################

# Hostname of the CE
CE_HOST=grid-creamce1.$MY_DOMAIN

##############################
# CREAMCE configuration      #
##############################
CREAM_DB_PASSWORD=$MYSQL_PASSWORD
CREAM_DB_USER=creamdb

############################
# SubCluster configuration #
############################

# Name of the processor model as defined by the vendor 
# for the Worker Nodes in a SubCluster.
CE_CPU_MODEL=Xeon

# Name of the processor vendor 
# for the Worker Nodes in a SubCluster
CE_CPU_VENDOR=intel

# Processor clock speed expressed in MHz 
# for the Worker Nodes in a SubCluster.
CE_CPU_SPEED=2400

# For the following variables please check:
# http://goc.grid.sinica.edu.tw/gocwiki/How_to_publish_the_OS_name
#
# Operating system name used on the Worker Nodes 
# part of the SubCluster.
CE_OS="Scientific Linux"

# Operating system release used on the Worker Nodes
# part of the SubCluster.
CE_OS_RELEASE=5.4

# Operating system version used on the Worker Nodes
# part of the SubCluster.
CE_OS_VERSION="SL"

# Platform Type of the WN in the SubCluster
# Check: http://goc.grid.sinica.edu.tw/gocwiki/How_to_publish_my_machine_architecture 
CE_OS_ARCH=x86_64

# Total physical memory of a WN in the SubCluster
# expressed in Megabytes.
CE_MINPHYSMEM=24000

# Total virtual memory of a WN in the SubCluster
# expressed in Megabytes.
CE_MINVIRTMEM=4000

# Total number of real CPUs/physical chips in 
# the SubCluster, including the nodes part of the 
# SubCluster that are temporary down or offline. 
CE_PHYSCPU=112

# Total number of cores/hyperthreaded CPUs in 
# the SubCluster, including the nodes part of the 
# SubCluster that are temporary down or offline
CE_LOGCPU=448

# Number of Logical CPUs (cores) of the WN in the 
# SubCluster
CE_SMPSIZE=8

# Performance index of your fabric in SpecInt 2000
CE_SI00=1963

# Performance index of your fabric in SpecFloat 2000
CE_SF00=1823

# Set this variable to either TRUE or FALSE to express 
# the permission for direct outbound connectivity 
# for the WNs in the SubCluster
CE_OUTBOUNDIP=TRUE

# Set this variable to either TRUE or FALSE to express 
# the permission for inbound connectivity 
# for the WNs in the SubCluster
CE_INBOUNDIP=TRUE

# Space separated list of software tags supported by the site
# e.g. CE_RUNTIMEENV="LCG-2 LCG-2_1_0 LCG-2_1_1 LCG-2_2_0 GLITE-3_0_0 GLITE-3_1_0 R-GMA"
CE_RUNTIMEENV="LCG-2 LCG-2_1_0 LCG-2_1_1 LCG-2_2_0 GLITE-3_0_0 GLITE-3_1_0 R-GMA"

# For the following variables, please check more detailed information in:
# https://twiki.cern.ch/twiki/bin/view/LCG/Site-info_configuration_variables#site_info_def
#
# The following values must be defined by the sys admin:
# - CPUScalingReferenceSI00=<referenceCPU-SI00> 
# - Share=<vo-name>:<vo-share> (optional, multiple definitons) 
CE_CAPABILITY="CPUScalingReferenceSI00=1963"

# The following values must be defined by the sys admin:
# - Cores=value
# - value-HEP-SPEC06 (optional), where value is the CPU power computed
#   using the HEP-SPEC06 benchmark
CE_OTHERDESCR="Cores=4"


########################################
# Batch server configuration variables #
########################################

# Hostname of the Batch server
# Change this if your batch server is not installed 
# in the same host of the CE
BATCH_SERVER=grid-pbs1.$MY_DOMAIN

# Jobmanager specific settings. Please, define:
# lcgpbs, lcglfs, lcgsge or lcgcondor
JOB_MANAGER=lcgpbs

# torque, lsf, sge or condor
CE_BATCH_SYS=torque
BATCH_LOG_DIR=/var/lib/torque
BATCH_VERSION=torque-2.5.7

################################
# APEL configuration variables #
################################

# Database password for the APEL DB.
APEL_DB_PASSWORD=$MYSQL_PASSWORD

##############################
# RB configuration variables #
##############################

# Hostname of the RB
RB_HOST=rb106.cern.ch

###############################
# WMS configuration variables #
###############################

# Hostname of the WMS
WMS_HOST=wms105.cern.ch

###################################
# myproxy configuration variables #
###################################

# Hostname of the PX
PX_HOST=myproxy.cern.ch

################################
# RGMA configuration variables #
################################

# Hostname of the RGMA server
MON_HOST=grid-apel1.$MY_DOMAIN
GRIDICE_SERVER_HOST=${MON_HOST}
JAVA_LOCATION=/usr/lib/jvm/java-openjdk

###################################
# FTS configuration variables #
###################################

# FTS endpoint
#FTS_SERVER_URL="https://fts.${MY_DOMAIN}:8443/path/glite-data-transfer-fts"

###############################
# DPM configuration variables #
###############################

# Hostname of the DPM head node 
#DPM_HOST="my-dpm.$MY_DOMAIN"

########################
# SE general variables #
########################

# Space separated list of SEs hostnames
SE_LIST="grid-se1.$MY_DOMAIN"

# Space separated list of SE hosts from SE_LIST containing 
# the export directory from the Storage Element and the 
# mount directory common to the worker nodes that are part 
# of the Computing Element. If any of the SEs in SE_LIST 
# does not support the mount concept, do not define 
# anything for that SE in this variable. If this is the case 
# for all the SEs in SE_LIST then put the value "none"  
SE_MOUNT_INFO_LIST="grid-se1.$MY_DOMAIN:/pnfs"
SE_MOUNT_INFO_LIST="none"

# Variable necessary to configure Gridview service client on the SEs.
# It sets the location and filename of the gridftp server logfile on 
# different types of SEs. Needed gridftp logfile for gridview is the 
# netlogger file which contain info for each transfer (created with
# -Z/-log-transfer option for globus-gridftp-server). 
# Ex: DATE=20071206082249.108921 HOST=hostname.cern.ch PROG=globus-gridftp-server 
# NL.EVNT=FTP_INFO START=20071206082248.831173 USER=atlas102 FILE=/storage/atlas/ 
# BUFFER=0 BLOCK=262144 NBYTES=330 VOLUME=/ STREAMS=1 STRIPES=1 DEST=[127.0.0.1] 
# TYPE=LIST CODE=226
# Default locations for DPM: /var/log/dpm-gsiftp/dpm-gsiftp.log
#            and SE_classic: /var/log/globus-gridftp.log
#SE_GRIDFTP_LOGFILE=path_to_gridftp_logfile.log
##################################
# dcache configuration variables #
##################################

# dCache-specific settings
# ignore if you are not running d-cache

# Your dcache admin node
DCACHE_ADMIN="grid-se1.$MY_DOMAIN"

# Pools must include host:/absolutePath and may optionally include
# size host:size:/absolutePath if the size is not set the pool will 
# fill the partition it is installed upon. size cannot be smaller 
# than 4 (Gb) unless you are an expert.

DCACHE_POOLS="grid-dp1.$MY_DOMAIN:5:/dpool1 grid-dp2.$MY_DOMAIN:5:/dpool1"

# Optional

# For large sites the load on the admin-node is a limiting factor. Pnfs
# accounts for a lot of this load and so can be placed on a different
# node to balance the load better.


# Set DCACHE_DOOR_* to "off" if you dont want the door to start on any host
#

DCACHE_DOOR_SRM="$DCACHE_ADMIN"
DCACHE_DOOR_GSIFTP="grid-dp1.$MY_DOMAIN grid-dp2.$MY_DOMAIN"
DCACHE_DOOR_GSIDCAP="grid-dp1.$MY_DOMAIN grid-dp2.$MY_DOMAIN"
DCACHE_DOOR_DCAP="grid-dp1.$MY_DOMAIN grid-dp2.$MY_DOMAIN"
# DCACHE_DOOR_GSIFTP="door_node1[:port] door_node2[:port]"
# DCACHE_DOOR_GSIDCAP="door_node1[:port] door_node2[:port]"
# DCACHE_DOOR_DCAP="door_node1[:port] door_node2[:port]"
# DCACHE_DOOR_XROOTD="door_node1[:port] door_node2[:port]"
DCACHE_DOOR_LDAP=$DCACHE_ADMIN
# DCACHE_DOOR_XROOTD="door_node1[:port] door_node2[:port]"
DCACHE_DOOR_XROOTD="grid-dp1.$MY_DOMAIN grid-dp2.$MY_DOMAIN"

# This option sets the pnfs server it defaults to the admin node if 
# not stated.
#
# DCACHE_PNFS_SERVER="pnfs_node"
#
# Sets the portrange for dcache as a GSIFTP server in "passive" mode
#
# DCACHE_PORT_RANGE_PROTOCOLS_SERVER_GSIFTP=50000,52000
#
# Sets the portrange for dcache as a (GSI)DCAP and xrootd server in 
# "passive" mode
#
# DCACHE_PORT_RANGE_PROTOCOLS_SERVER_MISC=60000,62000
#
# Sets the portrange for dcache as a GSIFTP client in "active" mode
#
# DCACHE_PORT_RANGE_PROTOCOLS_CLIENT_GSIFTP=33115,33215

# This option sets the pnfs server it defaults to the admin node if 
# not stated.
#
# DCACHE_PNFS_SERVER="pnfs_node"
#
# Sets the portrange for dcache as a GSIFTP server in "passive" mode
#
# DCACHE_PORT_RANGE_PROTOCOLS_SERVER_GSIFTP=50000,52000
#
# Sets the portrange for dcache as a (GSI)DCAP and xrootd server in 
# "passive" mode
#
# DCACHE_PORT_RANGE_PROTOCOLS_SERVER_MISC=60000,62000
#
# Sets the portrange for dcache as a GSIFTP client in "active" mode
#
# DCACHE_PORT_RANGE_PROTOCOLS_CLIENT_GSIFTP=33115,33215

# Only change if your site has an existing D-Cache installed
# To a different storage root.
DCACHE_PNFS_VO_DIR="/pnfs/${MY_DOMAIN}/data"

# Set to "yes" only if YAIM shall reset the dCache configuration,
# or install DCache for the first time.
# i.e. if you want YAIM to configure dCache - WARNING:
# this may wipe out any dCache parameters previously configured!
RESET_DCACHE_CONFIGURATION=yes

# Set to "yes" only if YAIM shall reset the dCache nameserver,
# Or install DCache for the first time.
# i.e. if you want YAIM to clear the content of dCache - WARNING:
# this may wipe out any dCache files previously stored!
RESET_DCACHE_PNFS=yes

# Set to "yes" only if YAIM shall reset the dCache Databases,
# or install DCache for the first time.
# i.e. if you want YAIM to clear the metadata of dCache - WARNING:
# this may wipe out any dCache files names previously stored!
# Leaving your system without any way to reestablish which files 
# are stored.
RESET_DCACHE_RDBMS=yes


################################
# BDII configuration variables #
################################

# Hostname of the top level BDII
BDII_HOST=grid-topbdii.$MY_DOMAIN

# Hostname of the site BDII
SITE_BDII_HOST=grid-bdii1.$MY_DOMAIN

# Uncomment this variable if you want to define a list of
# top level BDIIs to support the automatic failover in the GFAL clients 
# BDII_LIST=my-bdii1.$MY_DOMAIN:port1[,my-bdii22.$MY_DOMAIN:port2[...]] 


################################
# VOMS configuration variables #
################################

VOMS_HOST=grid-voms.$MY_DOMAIN
VOMS_ADMIN_SMTP_HOST=$VOMS_HOST
VOMS_ADMIN_MAIL=$SITE_EMAIL

## VOMS DB configurations
VOMS_DB_HOST=$VOMS_HOST

## VO specific configurations for VOMS
VO_ATLAS_VOMS_DB_NAME=atlas
VO_ATLAS_VOMS_DB_USER=atlas
VO_ATLAS_VOMS_DB_PASS=$MYSQL_PASSWORD
VO_ATLAS_VOMS_PORT=15001

VO_DTEAM_VOMS_DB_NAME=dteam
VO_DTEAM_VOMS_DB_USER=dteam
VO_DTEAM_VOMS_DB_PASS=$MYSQL_PASSWORD
VO_DTEAM_VOMS_PORT=15004

VO_OPS_VOMS_DB_NAME=ops
VO_OPS_VOMS_DB_USER=ops
VO_OPS_VOMS_DB_PASS=$MYSQL_PASSWORD
VO_OPS_VOMS_PORT=15009

VO_AEUGO_VOMS_DB_NAME=aeugo
VO_AEUGO_VOMS_DB_USER=aeugo
VO_AEUGO_VOMS_DB_PASS=$MYSQL_PASSWORD
VO_AEUGO_VOMS_PORT=15087

MY_CA_DN="/C=MU/O=PowerGrid/CN=MarsUnion-CA"
MY_VOMS_HOST_DN="/C=MU/O=PowerGrid/CN=host/$VOMS_HOST"



##############################
# VO configuration variables #
##############################
# If you are configuring a DNS-like VO, please check
# the following URL: https://twiki.cern.ch/twiki/bin/view/LCG/YaimGuide400#vo_d_directory

# Space separated list of VOs supported by your site
VOS="atlas dteam ops aeugo"

# Prefix of the experiment software directory in your CE
VO_SW_DIR=/exp_soft

# Space separated list of queues configured in your CE
QUEUES=${VOS}

# For each queue defined in QUEUES, define a _GROUP_ENABLE variable 
# which is a space separated list of VO names and VOMS FQANs:
# Ex.: MYQUEUE_GROUP_ENABLE="ops atlas cms /cms/Higgs /cms/ROLE=production"
# In QUEUE names containing dots and dashes replace them with underscore:
# Ex.: QUEUES="my.test-queue"
#      MY_TEST_QUEUE_GROUP_ENABLE="ops atlas"
#<queue-name>_GROUP_ENABLE="fqan1 [fqan2 [...]]"

# Optional variable to define the default SE used by the VO.
# Define the SE hostname if you want a specific SE to be the default one.
# If this variable is not defined, the first SE in SE_LIST will be used
# as the default one.  
# VO_<vo_name>_DEFAULT_SE=vo-default-se

# Optional variable to define a list of LBs used by the VO.
# Define a space separated list of LB hostnames.
# If this variable is not defined LB_HOST will be used.
# VO_<vo_name>_LB_HOSTS="vo-lb1 [vo-lb2 [...]]" 

# Optional variable to automatically add wildcards per FQAN 
# in the LCMAPS gripmap file and groupmap file. Set it to 'yes' 
# if you want to add the wildcards in your VO. Do not define it 
# or set it to 'no' if you do not want to configure wildcards in your VO. 
# VO_<vo_name>_MAP_WILDCARDS=no

# Optional variable to define the Myproxy server supported by the VO. 
# Define the Myproxy hostname if you want a specific Myproxy server. 
# If this variable is not defined PX_HOST will be used. 
# VO_<vo_name>_PX_HOST=vo-myproxy

# Optional variable to define a list of RBs used by the VO.
# Define a space separated list of RB hostnames.
# If this variable is not defined RB_HOST will be used.
# VO_<vo_name>_RBS="vo-rb1 [vo-rb2 [...]]" 

# Area on the WN for the installation of the experiment software. 
# If on the WNs a predefined shared area has been mounted where 
# VO managers can pre-install software, then these variable 
# should point to this area. If instead there is not a shared 
# area and each job must install the software, then this variables 
# should contain a dot ( . ). Anyway the mounting of shared areas, 
# as well as the local installation of VO software is not managed 
# by yaim and should be handled locally by Site Administrators. 
# VO_<vo_name>_SW_DIR=wn_exp_soft_dir

# This variable contains the vomses file parameters needed 
# to contact a VOMS server. Multiple VOMS servers can be given 
# if the parameters are enclosed in single quotes. 
# VO_<vo_name>_VOMSES="'vo_name voms_server_hostname port voms_server_host_cert_dn vo_name' ['...']"

# DN of the CA that signs the VOMS server certificate. 
# Multiple values can be given if enclosed in single quotes. 
# Note that there must be as many entries as in the VO_<vo-name>_VOMSES variable. 
# There is a one to one relationship in the elements of both lists, 
# so the order must be respected
# VO_<vo_name>_VOMS_CA_DN="'voms_server_ca_dn' ['...']"

# A list of the VOMS servers used to create the DN grid-map file. 
# Multiple values can be given if enclosed in single quotes.
# VO_<vo_name>_VOMS_SERVERS="'vomss://<host-name>:8443/voms/<vo-name>?/<vo-name>' ['...']"

# Optional variable to define a list of WMSs used by the VO.
# Define a space separated list of WMS hostnames.
# If this variable is not defined WMS_HOST will be used.
# VO_<vo_name>_WMS_HOSTS="vo-wms1 [vo-wms2 [...]]"

# Optional variable to create a grid-mapfile with mappings to ordinary
# pool accounts, not containing mappings to special users.
# - UNPRIVILEGED_MKGRIDMAP=no or undefined, will contain
# special users if defined in groups.conf
# - UNPRIVILEGED_MKGRIDMAP=yes, will create a grid-mapfile
# containing only mappings to ordinary pool accounts.
# VO_<vo_name>_UNPRIVILEGED_MKGRIDMAP=no

# gLite pool account home directory for the user accounts specified in USERS_CONF.
# Define this variable if you would like to use a directory different than /home.
# VO_<vo_name>_USER_HOME_PREFIX=/pool_account_home_dir

# Examples for the following VOs are included below:
#
# atlas
# alice
# lhcb
# cms
# dteam
# biomed
# ops
# aeugo (Anti Earth Union GrOup) = A test VO in MarsUnion CA
# 
#
# VOs should check the CIC portal http://cic.in2p3.fr for the VO ID card information
#
#
#########
# atlas #
#########
ATLAS_GROUP_ENABLE="atlas"
VO_ATLAS_SW_DIR=$VO_SW_DIR/atlas
VO_ATLAS_DEFAULT_SE=$SE_HOST
VO_ATLAS_STORAGE_DIR=$CLASSIC_STORAGE_DIR/atlas
VO_ATLAS_VOMS_SERVERS='vomss://voms.cern.ch:8443/voms/atlas?/atlas/'
VO_ATLAS_VOMSES="'atlas $VOMS_HOST 15001 $MY_VOMS_HOST_DN atlas 24' 'atlas lcg-voms.cern.ch 15001 /DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch atlas 24' 'atlas voms.cern.ch 15001 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch atlas 24'"
VO_ATLAS_VOMS_CA_DN="'$MY_CA_DN' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority'"
VO_ATLAS_RBS="atlasrb1.cern.ch atlasrb2.cern.ch"


#########
# dteam #
#########
DTEAM_GROUP_ENABLE="dteam"
VO_DTEAM_SW_DIR=$VO_SW_DIR/dteam
VO_DTEAM_DEFAULT_SE=$SE_HOST
VO_DTEAM_STORAGE_DIR=$CLASSIC_STORAGE_DIR/dteam
VO_DTEAM_VOMS_SERVERS='vomss://voms.cern.ch:8443/voms/dteam?/dteam/'
VO_DTEAM_VOMSES="'dteam $VOMS_HOST 15004 $MY_VOMS_HOST_DN dteam 24' 'dteam lcg-voms.cern.ch 15004 /DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch dteam 24' 'dteam voms.cern.ch 15004 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch dteam 24'"
VO_DTEAM_VOMS_CA_DN="'$MY_CA_DN' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority'"

#######
# ops #
#######
OPS_GROUP_ENABLE="ops"
VO_OPS_SW_DIR=$VO_SW_DIR/ops
VO_OPS_DEFAULT_SE=$SE_HOST
VO_OPS_STORAGE_DIR=$CLASSIC_STORAGE_DIR/ops
VO_OPS_VOMS_SERVERS="vomss://voms.cern.ch:8443/voms/ops?/ops/"
VO_OPS_VOMSES="'ops $VOMS_HOST 15009 $MY_VOMS_HOST_DN ops 24' 'ops lcg-voms.cern.ch 15009 /DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch ops 24' 'ops voms.cern.ch 15009 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch ops 24'"
VO_OPS_VOMS_CA_DN="'$MY_CA_DN' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority'"


#########
# aeugo #
#########
AEUGO_GROUP_ENABLE="aeugo"
VO_AEUGO_SW_DIR=$VO_SW_DIR/aeugo
VO_AEUGO_DEFAULT_SE=$SE_HOST
VO_AEUGO_STORAGE_DIR=$CLASSIC_STORAGE_DIR/aeugo
VO_AEUGO_VOMS_SERVERS="vomss://$VOMS_HOST:8443/voms/aeugo?/aeugo/"
VO_AEUGO_VOMSES="'aeugo $VOMS_HOST 15087 $MY_VOMS_HOST_DN aeugo 24'"
VO_AEUGO_VOMS_CA_DN="'$MY_CA_DN'"


#--------------------------------
# For Site BDII
#--------------------------------
#BDII_REGIONS="CE SITE_BDII MON WMS"
BDII_REGIONS="CE SITE_BDII MON"

# BDII_<REGION>_URL="ldap://$<RESOURCE hostname>:2170/mds-vo-name=resource,o=grid"
BDII_CE_URL="ldap://$CE_HOST:2170/mds-vo-name=resource,o=grid"
BDII_SITE_BDII_URL="ldap://$SITE_BDII_HOST:2170/mds-vo-name=resource,o=grid"
BDII_MON_URL="ldap://$MON_HOST:2170/mds-vo-name=resource,o=grid"
#BDII_WMS_URL="ldap://$WMS_HOST:2170/mds-vo-name=resource,o=grid"

SITE_DESC="German HEP Tier2, Portable-Grid, University of Goettingen, Germany"
SITE_WEB="http://www.portable-grid.de/"
SITE_SECURITY_EMAIL="Gen.Kawamura@cern.ch"
SITE_SUPPORT_EMAIL="Gen.Kawamura@cern.ch"
SITE_OTHER_GRID="DECH"


########################################
# StoRM configuration variables       #
########################################
STORM_HOST=grid-storm1.$MY_DOMAIN
STORM_PORT=8444
STORM_USER=storm
STORM_DB_USER=storm
STORM_DB_PWD=stormdb
STORM_DB_HOST=localhost
STORM_DEFAULT_ROOT=/storm
STORM_AUTH_POLICY=permit
STORM_AP_LIST=$VOS
STORM_STORAGEAREA_LIST=$VOS



STORM_ATLAS_VONAME=atlas
STORM_DTEAM_VONAME=dteam
STORM_OPS_VONAME=ops

STORM_ATLASLOCALGROUPDISK_VONAME=atlas
STORM_ATLASLOCALGROUPDISK_ACCESSPOINT=/atlas/atlaslocalgroupdisk
STORM_ATLASLOCALGROUPDISK_ROOT=$STORM_DEFAULT_ROOT/atlas/atlaslocalgroupdisk
STORM_ATLASLOCALGROUPDISK_TOKEN=ATLASLOCALGROUPDISK

NTP_HOSTS_IP="1.de.pool.ntp.org"
STORM_FRONTEND_HOST_LIST=$STORM_HOST
STORM_BACKEND_HOST=$STORM_HOST

STORM_ATLAS_ONLINE_SIZE=1
STORM_DTEAM_ONLINE_SIZE=1
STORM_OPS_ONLINE_SIZE=1
STORM_AEUGO_ONLINE_SIZE=1



########################################
# ARGUS configuration variables       #
########################################

ARGUS_HOST=grid-argus.$MY_DOMAIN
PAP_ADMIN_DN="/C=MU/O=PowerGrid/OU=local/CN=Hyper User" # Gen CA


########################################
# GLEXEC configuration variables       #
########################################
# Change this to the correct endpoint
ARGUS_PEPD_ENDPOINTS="https://$ARGUS_HOST:8154/authz"
# Change this to the correct resource field in the PAP policy
GLEXEC_WN_PEPC_RESOURCEID="http://authz-interop.org/xacml/resource/resource-type/wn"
# Change this to the correct action in the PAP policy
GLEXEC_WN_PEPC_ACTIONID="http://glite.org/xacml/action/execute"

GLEXEC_WN_ARGUS_ENABLED=yes
GLEXEC_WN_SCAS_ENABLED="no"

# Change this to the list of users that may run gLExec 
GLEXEC_EXTRA_WHITELIST=".atlas, .atlprd, .ops, .opssgm"

GLEXEC_WN_OPMODE="setuid"
GLEXEC_WN_USE_LCAS="no"
#GLEXEC_WN_LCAS_DEBUG_LEVEL="0"

GLEXEC_WN_LOG_LEVEL=3
GLEXEC_WN_LCMAPS_DEBUG_LEVEL=3
GLEXEC_WN_LOG_DESTINATION=syslog
#GLEXEC_WN_LOG_DESTINATION=file
#GLEXEC_WN_LOG_FILE=/var/log/glexec/glexec.log
#GLEXEC_WN_LCASLCMAPS_LOG=/var/log/glexec/lcas_lcmaps.log

GLEXEC_WN_INPUT_LOCK=flock
GLEXEC_WN_TARGET_LOCK=flock
